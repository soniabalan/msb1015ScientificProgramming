---
title: "Birds in Romania"
author: Sonia BÄƒlan
output: html_document
date: "2023-09/10"

Data Info:
  Username: soniaa.balan
  Format: SIMPLE_CSV
  Download key: 0018307-230828120925497
  Created: 2023-09-16T09:31:59.821+00:00
  Citation Info:  
    Please always cite the download DOI when using this data.
    https://www.gbif.org/citation-guidelines
    DOI: 10.15468/dl.y8qsxq
    Citation:
    GBIF Occurrence Download https://doi.org/10.15468/dl.y8qsxq Accessed from R via rgbif(https://github.com/ropensci/rgbif) on 2023-09-16

GBIF Query used to retreive this data (using occ_download)):
  pred("taxonKey", 212), # aves - all bird species
  pred("country","RO"), # Romania
  format = "SIMPLE_CSV",

---

# Setup 
```{r} 
source('0_Dependencies.R')

# set wrok environment
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```

# Retrieve Data from GBIF using their API
```{r}
# retrieve downloaded data
data_raw<- occ_download_get('0018307-230828120925497') %>%
  occ_download_import()

# remove irrelevant and uninformative (no available data) columns
remove_vars = c("datasetKey",
                "occurrenceID",
                "kingdom",
                "phylum",
                "class",
                "verbatimScientificNameAuthorship",
                "countryCode",
                "publishingOrgKey",
                "coordinatePrecision",
                "elevation",
                "elevationAccuracy",
                "depth",
                "depthAccuracy",
                "taxonKey",
                "speciesKey",
                "catalogNumber",
                "recordNumber",
                "license",
                "rightsHolder",
                "establishmentMeans",
                "lastInterpreted",
                "recordedBy",
                "typeStatus",
                "mediaType",
                "issue",
                "institutionCode",
                "identifiedBy",
                "publishingOrgKey",
                "datasetKey",
                "occurrenceID",
                "taxonRank",
                "scientificName",
                "locality",
                "day",
                "month",
                "year",
                "collectionCode",
                "coordinateUncertaintyInMeters",
                "occurrenceStatus",
                "infraspecificEpithet",
                "basisOfRecord")

data = data_raw %>% select(-remove_vars)
```

# General Cleaning
  - removing rows without any taxonomic data
  - replace NA and 0 values from individualCounts with 1
```{r}
# remove samples with no informative taxonomic data
data <- data[-which(data$order == ""), ]
data <- data[!is.na(data$order), ]

# replace NAs in count data by 1
data$individualCount <- replace(data$individualCount, is.na(data$individualCount), 1)
data$individualCount <- replace(data$individualCount, data$individualCount == 0, 1)
```
# Location Data
  Using information from: city/county database from https://github.com/romania/localitati 
  This data contains a "dictionary" of Romanian cities, their province and coordinates
```{r}
# load information about Romanian administrative units 
cities_list <- read.csv("Data/orase.csv", stringsAsFactors = T)
cities_list = cities_list %>% select(c("X",
                                       "Y",
                                       "NUME",
                                       "JUDET"))


# rename columns
colnames(cities_list) = c("decimalLongitude","decimalLatitude","locality","stateProvince")

# remove stateProvince levels that are NOT real provinces in Romania
unwanted_levels <- setdiff(levels(data$stateProvince), levels(cities_list$stateProvince))
levels(data$stateProvince)[levels(data$stateProvince) %in% unwanted_levels] <- NA

# format cities library
coord_by_province <- cities_list %>%
  group_by(stateProvince) %>%
  summarize(
    lon_list = sort(decimalLongitude),
    lat_list = sort(decimalLatitude)
  )
```
  Replace Province Labels Based on Coordinates Using 'rgeos'
```{r}
'The following lines use the \'rgeos\' package, which was archived earlier this year (2023). This section would most likely not work. The stateProvince labels were exported on a previous run and can be loaded directly (in the next code section)'

# # only look at data without province labels
# missing_indices <- which(is.na(data$stateProvince) &
#                            !is.na(data$decimalLatitude) &
#                            !is.na(data$decimalLongitude))
# 
# data$decimalLatitude = as.numeric(data$decimalLatitude)
# data$decimalLongitude = as.numeric(data$decimalLongitude)
#                                    
# # Create SpatialPointsDataFrame from coordinates in the data
# cities_sp <- SpatialPointsDataFrame(coords = data[missing_indices, c("decimalLongitude", "decimalLatitude")], data=data[missing_indices,])
# 
# 
# ##  Create a "dictionary" of Polygons ##
# province_list <- unique(coord_by_province$stateProvince)
# prov_boundaries <- list()
# 
# # Find the perimeter for each cloud of points (each point representing a city) that represents the boundary of a province
# for (province in province_list) {
#   prov_data <- coord_by_province[coord_by_province$stateProvince == province, ]
# 
#   # find the convex hull of each province
#   coord <- prov_data[, c("lon_list", "lat_list")]
#   points <- SpatialPoints(coord)
#   convex_hull <- gConvexHull(points)
#   polygon_object <- convex_hull@polygons[[1]]
#   polygon_object@ID <- as.character(province)
# 
#   # Add the boundary to the list
#   prov_boundaries[[province]] <- polygon_object
# }
# 
# # creates a SpatialPolygons object
# states_sp <- SpatialPolygons(prov_boundaries)
# states_sp_buffered <- gBuffer(states_sp, width = 0)
# 
# states_spdf <- SpatialPolygonsDataFrame(
#   states_sp,
#   data = data.frame(stateProvince = province_list),
#   match.ID = F
# )
# 
# states_tree <- gUnaryUnion(states_spdf, id = (stateProvince = province_list))
# 
# #
# 
# # Loop through the missing_indices and use spatial index to find the state
# for (i in 1:length(missing_indices)) {
#   point <- cities_sp[i, ]
#   hits <- over(point, states_tree)
# 
#   if (!is.null(hits) & !is.na(hits)) {
#     data$stateProvince[missing_indices[i]] <- province_list[hits]
#   }
# }
# 
# print(summary(data[missing_indices,]))
# print(length(missing_indices))
# print(sum(is.na(data$stateProvince)))

# # intermediary save
# write.csv(data$stateProvince, "Data/geography_labels.csv", row.names=FALSE)
```
  geographical labels upload (if 'rgeos' is not available for install)
```{r}
stateProvince_labels <- read.csv("Data/geography_labels.csv")
data$stateProvince <- stateProvince_labels$stateProvince

```
# Taxonomy Labels
  - aggregate information about taxonomical labels from duplicate columns containing the same information at different rows in different columns
```{r}
# Renaming verbatim scientific name levels
lvl_verbatim = levels(data$verbatimScientificName)
verbatim_sp <- lapply(lvl_verbatim, function(x) word(x, 1, 2))
levels(data$verbatimScientificName) = unlist(verbatim_sp)

# add species information from verbatimScientificName 
data$species <- as.character(data$species)
data[which(data$species==""),"species"] <-   as.vector(data[(data$species==""),"verbatimScientificName"])
data$species <- as.factor(data$species)

# add genus information from species
gen <- lapply(data[data$genus=="" & !is.na(data$species),"species"], function(x) word(x, 1))

# perform replacement
data$genus <- as.character(data$genus)
data[which(data$genus=="" & !is.na(data$species)),"genus"] <- as.vector(gen)
data$genus <- as.factor(data$genus)

# sanity checks
'remaining non-labeled rows for genus:'
print(sum(is.na(data$genus))) 

# remove duplicate column
data = data %>% select(-c("verbatimScientificName"))
```
  Dates
```{r}
'Missing date entries before replacement:'
print(sum(is.na(data$eventDate)))


data$eventDate <- replace(data$eventDate, is.na(data$eventDate), data$dateIdentified)

'Remaining missing date entries:'
print(sum(is.na(data$eventDate)))

# removie duplicate column
data = data %>% select(-c("dateIdentified"))
```
# Check remaining missingness
```{r}
# keep only features of interest
data_clean <- data

feature_missingness <- colSums(is.na(data_clean))
indiv_missingness<- as.data.frame(rowSums(is.na(data_clean)))
colnames(indiv_missingness) <- c("total")

library(ggplot2)
ggplot(indiv_missingness, aes(total)) +
  geom_histogram() +
  labs(title = "Row-wise missingness",
       x = "Nr. missing values",
       y = "Count")
```
Based on the row-wise missingness, since most rows have all the information, this dataset is considered sufficient for visualisation. 

# Export Processed Data
```{r}
write.csv(data_clean, "Data/data_clean.csv", row.names=FALSE)
```
