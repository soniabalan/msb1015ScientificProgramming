---
title: "Birds in Romania"
output: html_document
date: "2023-09/10"

Data Info:
  Username: soniaa.balan
  Format: SIMPLE_CSV
  Download key: 0018307-230828120925497
  Created: 2023-09-16T09:31:59.821+00:00
  Citation Info:  
    Please always cite the download DOI when using this data.
    https://www.gbif.org/citation-guidelines
    DOI: 10.15468/dl.y8qsxq
    Citation:
    GBIF Occurrence Download https://doi.org/10.15468/dl.y8qsxq Accessed from R via rgbif(https://github.com/ropensci/rgbif) on 2023-09-16

---

# Setup 
```{r} 
# Libraries install
# install.packages("rstudioapi")
# install.packages("rgbif")
# install.packages("dplyr")
# install.packages("stringr")
# install.packages("stringdist")
# install.packages("microbenchmark")
# install.packages("doParallel")


# Libraries loading
library(rgbif)
library(dplyr)
library(stringr)
library(rstudioapi)
library(stringdist)
library(microbenchmark)
library(doParallel)

# set wrok environment
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```

# Retrieve Data from GBIF using their API
```{r}
# # download occurrence data
# occ_download(
#   # filters applied
#   pred("taxonKey", 212), # aves - all bird species
#   pred("country","RO"), # Romania
#   format = "SIMPLE_CSV",
#   # accreditation
#   user = "soniaa.balan",
#   pwd = "not_a_real_password", # for safety reasons, the password is not included in this script
#   email = "soniaa.balan@gmail.com"
# )

# # check download status - also returns Download information
# occ_download_wait('0018307-230828120925497')

# retrieve downloaded data
data_raw<- occ_download_get('0018307-230828120925497') %>%
  occ_download_import()

# remove irrelevant and uninformative (no available data) columns
remove_vars = c("datasetKey",
                "occurrenceID",
                "kingdom",
                "phylum",
                "class",
                "verbatimScientificNameAuthorship",
                "countryCode",
                "publishingOrgKey",
                "coordinatePrecision",
                "elevation",
                "elevationAccuracy",
                "depth",
                "depthAccuracy",
                "taxonKey",
                "speciesKey",
                "catalogNumber",
                "recordNumber",
                "license",
                "rightsHolder",
                "establishmentMeans",
                "lastInterpreted",
                "recordedBy",
                "typeStatus",
                "mediaType",
                "issue",
                "institutionCode",
                "identifiedBy",
                "publishingOrgKey",
                "datasetKey",
                "occurrenceID",
                "taxonRank",
                "scientificName")

data = data_raw %>% select(-remove_vars)
```

# Data Cleaning ----------------------------------------------------------------
  General Cleaning
```{r}
# remove samples with no informative taxonomic data
data <- data[-which(data$order == ""), ]
data <- data[!is.na(data$order), ]

# factorize data stored as characters
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)],as.factor)
data$month <- as.factor(data$month)

# replace NAs in count data by 1
data$individualCount <- replace(data$individualCount, is.na(data$individualCount), 1)
data$individualCount <- replace(data$individualCount, data$individualCount == 0, 1)

# summaries of remaining columns - for debugging
for (var in colnames(data)) {
  print(summary(data[,var]))
}
```
  Taxonomy Labels
```{r}

##### Renaming verbatim scientific name levels #####
lvl_verbatim = levels(data$verbatimScientificName)
verbatim_sp <- lapply(lvl_verbatim, function(x) word(x, 1, 2))
levels(data$verbatimScientificName) = unlist(verbatim_sp)

# add species information from verbatimScientificName 
data$species <- as.character(data$species)
data[which(data$species==""),"species"] <-   as.vector(data[(data$species==""),"verbatimScientificName"])
data$species <- as.factor(data$species)

# add genus information from species
gen <- lapply(data[data$genus=="" & !is.na(data$species),"species"], function(x) word(x, 1))

data$genus <- as.character(data$genus)
data[which(data$genus=="" & !is.na(data$species)),"genus"] <- as.vector(gen)
data[data$genus=="","genus"] <- NA
data$genus <- as.factor(data$genus)

# sanity checks
print(which(data$species=="")) # there shouldn't be any more empty species
print(which(data$genus=="")) # there shouldn't be any more empty genus

data = data %>% select(-c("verbatimScientificName"))
```
  Dates
```{r}
print("before")
print(sum(is.na(data$eventDate)))


data$eventDate <- replace(data$eventDate, is.na(data$eventDate), data$dateIdentified)

# remaining NAs
print("after")
print(sum(is.na(data$eventDate)))

# data = data %>% select(-c("dateIdentified"))
```
  Location Data
  Using information from: city/county database from https://github.com/romania/localitati
```{r}
############################################################
cities_list <- read.csv("orase.csv", stringsAsFactors = T)
cities_list = cities_list %>% select(c("X",
                                       "Y",
                                       "NUME",
                                       "JUDET"))


# rename columns
colnames(cities_list) = c("decimalLongitude","decimalLatitude","locality","stateProvince")
# print(sum(data$stateProvince==""))

# remove stateProvince levels that are NOT real provinces in Romania
unwanted_levels <- setdiff(levels(data$stateProvince), levels(cities_list$stateProvince))
levels(data$stateProvince)[levels(data$stateProvince) %in% unwanted_levels] <- NA

# print(levels(data$stateProvince))
```
    - filter based on coordinates
```{r}
coord_by_province <- cities_list %>%
  group_by(stateProvince) %>%
  summarize(
    min_lon = min(decimalLongitude),
    max_lon = max(decimalLongitude),
    min_lat = min(decimalLatitude),
    max_lat = max(decimalLatitude)
  )

test = data[,c("stateProvince","decimalLongitude","decimalLatitude")]

'Unparallelized Version'
missing_indices <- which(is.na(data$stateProvince) &
                           !is.na(data$decimalLatitude) &
                           !is.na(data$decimalLongitude))
# test_indices = head(missing_indices)

for (i in missing_indices){
  region <- coord_by_province %>%
    filter(test$decimalLatitude[i] >= min_lat & test$decimalLatitude[i] <= max_lat &
           test$decimalLongitude[i] >= min_lon & test$decimalLongitude[i] <= max_lon) %>%
    select(stateProvince)
  if (length(region$stateProvince)>0){
    test$stateProvince[i] <- region$stateProvince
  }
}


'Paralelized Version'
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# filter_coordinates <- function(i){
#   region <- coord_by_province %>%
#     filter(test$decimalLatitude[i] >= min_lat & test$decimalLatitude[i] <= max_lat &
#            test$decimalLongitude[i] >= min_lon & test$decimalLongitude[i] <= max_lon) %>%
#     select(stateProvince)
#   # assign a new state province if and only if a single province was identified 
#   if (nrow(region)==1){
#     return(region$stateProvince)
#   } else {
#     return(NA)
#   }
# }
# 
# missing_indices <- which(is.na(data$stateProvince) &
#                            !is.na(data$decimalLatitude) &
#                            !is.na(data$decimalLongitude))
# # test_indices = head(missing_indices)
# state_provences <- character(length(missing_indices))
# state_provences <- foreach(i = missing_indices, .packages = c("dplyr")) %dopar% {
#   filter_coordinates(i)
# }
# # Stop the parallel
# stopCluster(cl)
# test$stateProvince[missing_indices] <- state_provences

print(length(missing_indices))
print(sum(is.na(test$stateProvince)))

```
    - filter based on locality name
```{r}

###########################################################
'The Paralelized Process'
num_cores <- 4

# Initialize a parallel
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Create a function to identify the province where an observation was made, based on a distance metric that compares the strings of locality
calculate_closest_city <- function(i, state_provences) {
  distances <- stringdist(data$locality[i], cities_list$locality)
  closest_index <- which.min(distances)
  # the function only returns a county if the distance is very small, otherwise returns NA
  if (distances[closest_index] < 0.5) {
    return(cities_list$stateProvince[closest_index])
  } else { 
    return(NA)
  }
}


# only perform the search for rows that don't already have province infromation, and that have locality information
missing_indices <- which(is.na(data$stateProvince) & data$locality!="")
state_provences <- character(length(missing_indices))

# the paralel loop process of identifying provinces
state_provences <- foreach(i = missing_indices, .packages = c("stringdist")) %dopar% {
  calculate_closest_city(i)
}

# Stop the parallel
stopCluster(cl)

# add the identified province data in the dataset
data$stateProvince[missing_indices] <- state_provences

print(sum(is.na(data$stateProvince)))
print(sum(data$stateProvince==""))


############################################################
'The un-parallelized version of the process'

# missing_indices <- which(data$stateProvince=="" & data$locality!="")
# state_provences <- character(length(missing_indices))
# 
# # time-consuming step - can still parallelize
# for (i in missing_indices) {
#   
#   distances <- stringdist(data$locality[i], cities_list$locality)
#   closest_index <- which.min(distances)
#   if (distances[closest_index] < 0.5) {
#     state_provences[i] <- cities_list$stateProvince[closest_index]
#   } else {
#     state_provences[i] <- NA 
#   }
# }
```
# Export Processed Data
```{r}
write.csv(data, "data_filled_gaps.csv", row.names=FALSE)
```
