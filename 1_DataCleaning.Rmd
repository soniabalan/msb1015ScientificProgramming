---
title: "Birds in Romania"
output: html_document
date: "2023-09/10"

Data Info:
  Username: soniaa.balan
  Format: SIMPLE_CSV
  Download key: 0018307-230828120925497
  Created: 2023-09-16T09:31:59.821+00:00
  Citation Info:  
    Please always cite the download DOI when using this data.
    https://www.gbif.org/citation-guidelines
    DOI: 10.15468/dl.y8qsxq
    Citation:
    GBIF Occurrence Download https://doi.org/10.15468/dl.y8qsxq Accessed from R via rgbif(https://github.com/ropensci/rgbif) on 2023-09-16

---

# Setup 
```{r} 
# Libraries install
# install.packages("rstudioapi")
# install.packages("rgbif")
# install.packages("dplyr")
# install.packages("stringr")
# install.packages("stringdist")
# install.packages("microbenchmark")
# install.packages("doParallel")
# install.packages("sp")
# install.packages("sf") # co=requisite of "rgeos", takes a long time to install
# install.packages("rgeos") 
# install.packages("httr")


# Libraries loading
library(rgbif)
library(dplyr)
library(stringr)
library(rstudioapi)
library(stringdist)
library(microbenchmark)
library(doParallel)
library(sp)
library(rgeos)

# set wrok environment
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```

# Retrieve Data from GBIF using their API
```{r}
# # download occurrence data
# occ_download(
#   # filters applied
#   pred("taxonKey", 212), # aves - all bird species
#   pred("country","RO"), # Romania
#   format = "SIMPLE_CSV",
#   # accreditation
#   user = "soniaa.balan",
#   pwd = "not_a_real_password", # for safety reasons, the password is not included in this script
#   email = "soniaa.balan@gmail.com"
# )

# # check download status - also returns Download information
# occ_download_wait('0018307-230828120925497')

# retrieve downloaded data
data_raw<- occ_download_get('0018307-230828120925497') %>%
  occ_download_import()

# remove irrelevant and uninformative (no available data) columns
remove_vars = c("datasetKey",
                "occurrenceID",
                "kingdom",
                "phylum",
                "class",
                "verbatimScientificNameAuthorship",
                "countryCode",
                "publishingOrgKey",
                "coordinatePrecision",
                "elevation",
                "elevationAccuracy",
                "depth",
                "depthAccuracy",
                "taxonKey",
                "speciesKey",
                "catalogNumber",
                "recordNumber",
                "license",
                "rightsHolder",
                "establishmentMeans",
                "lastInterpreted",
                "recordedBy",
                "typeStatus",
                "mediaType",
                "issue",
                "institutionCode",
                "identifiedBy",
                "publishingOrgKey",
                "datasetKey",
                "occurrenceID",
                "taxonRank",
                "scientificName")

data = data_raw %>% select(-remove_vars)
```

# Data Cleaning ----------------------------------------------------------------
  General Cleaning
```{r}
# remove samples with no informative taxonomic data
data <- data[-which(data$order == ""), ]
data <- data[!is.na(data$order), ]

# factorize data stored as characters
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)],as.factor)
data$month <- as.factor(data$month)

# replace NAs in count data by 1
data$individualCount <- replace(data$individualCount, is.na(data$individualCount), 1)
data$individualCount <- replace(data$individualCount, data$individualCount == 0, 1)

# summaries of remaining columns - for debugging
for (var in colnames(data)) {
  print(summary(data[,var]))
}
```
  Location Data
  Using information from: city/county database from https://github.com/romania/localitati
```{r}
############################################################
cities_list <- read.csv("orase.csv", stringsAsFactors = T)
cities_list = cities_list %>% select(c("X",
                                       "Y",
                                       "NUME",
                                       "JUDET"))


# rename columns
colnames(cities_list) = c("decimalLongitude","decimalLatitude","locality","stateProvince")
# print(sum(data$stateProvince==""))

# remove stateProvince levels that are NOT real provinces in Romania
unwanted_levels <- setdiff(levels(data$stateProvince), levels(cities_list$stateProvince))
levels(data$stateProvince)[levels(data$stateProvince) %in% unwanted_levels] <- NA

# print(levels(data$stateProvince))
```
    - filter based on coordinates
```{r}
coord_by_province <- cities_list %>%
  group_by(stateProvince) %>%
  summarize(
    lon_list = sort(decimalLongitude),
    lat_list = sort(decimalLatitude)
  )

test = data[,c("stateProvince","decimalLongitude","decimalLatitude")]

missing_indices <- which(is.na(data$stateProvince) &
                           !is.na(data$decimalLatitude) &
                           !is.na(data$decimalLongitude))
test_indices = head(missing_indices, n = 10)

##############################################
'Geography Package Version?'
# 
# Create SpatialPointsDataFrame from coordinates in the data
cities_sp <- SpatialPointsDataFrame(coords = test[missing_indices, c("decimalLongitude", "decimalLatitude")], data=test[missing_indices,])


##  Create a "dictionary" of Polygons ##
province_list <- unique(coord_by_province$stateProvince)
prov_boundaries <- list()

# Find the perimeter for each cloud of points (each point representing a city) that represents the boundary of a province
for (province in province_list) {
  prov_data <- coord_by_province[coord_by_province$stateProvince == province, ]
  
  # find the convex hull of each province
  coord <- prov_data[, c("lon_list", "lat_list")]
  points <- SpatialPoints(coord)
  convex_hull <- gConvexHull(points)
  polygon_object <- convex_hull@polygons[[1]]
  polygon_object@ID <- as.character(province)
  
  # Add the boundary to the list
  prov_boundaries[[province]] <- polygon_object
}

# creates a SpatialPolygons object
states_sp <- SpatialPolygons(prov_boundaries)
states_sp_buffered <- gBuffer(states_sp, width = 0)

states_spdf <- SpatialPolygonsDataFrame(
  states_sp,
  data = data.frame(stateProvince = province_list),
  match.ID = F
)

states_tree <- gUnaryUnion(states_spdf, id = (stateProvince = province_list))



# Loop through the missing_indices and use spatial index to find the state
for (i in 1:length(missing_indices)) {
  # print("i")
  # print(i)
  point <- cities_sp[i, ]
  # print('point')
  # print(point)
  # print('hits')
  hits <- over(point, states_tree)
  # print(hits)

  if (!is.null(hits) & !is.na(hits)) {
    test$stateProvince[missing_indices[i]] <- province_list[hits]
  }
}

print(summary(test[missing_indices,]))
print(length(missing_indices))
print(sum(is.na(test$stateProvince)))
  test$stateProvince[i] <- region$stateProvince

data$stateProvince <- test$stateProvince
# intermediary save
write.csv(test, "geography_labels.csv", row.names=FALSE)

```
    - filter based on locality name
```{r}

###########################################################
'The Paralelized Process'
num_cores <- 4

# Initialize a parallel
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Create a function to identify the province where an observation was made, based on a distance metric that compares the strings of locality
calculate_closest_city <- function(i, state_provences) {
  distances <- stringdist(data$locality[i], cities_list$locality)
  closest_index <- which.min(distances)
  # the function only returns a county if the distance is very small, otherwise returns NA
  if (distances[closest_index] < 0.5) {
    return(cities_list$stateProvince[closest_index])
  } else { 
    return(NA)
  }
}


# only perform the search for rows that don't already have province infromation, and that have locality information
missing_indices <- which(is.na(data$stateProvince) & data$locality!="")
state_provences <- character(length(missing_indices))

# the paralel loop process of identifying provinces
state_provences <- foreach(i = missing_indices, .packages = c("stringdist")) %dopar% {
  calculate_closest_city(i)
}

# Stop the parallel
stopCluster(cl)

# add the identified province data in the dataset
data$stateProvince[missing_indices] <- state_provences

print(sum(is.na(data$stateProvince)))
print(sum(data$stateProvince==""))


############################################################
'The un-parallelized version of the process'

# missing_indices <- which(data$stateProvince=="" & data$locality!="")
# state_provences <- character(length(missing_indices))
# 
# # time-consuming step - can still parallelize
# for (i in missing_indices) {
#   
#   distances <- stringdist(data$locality[i], cities_list$locality)
#   closest_index <- which.min(distances)
#   if (distances[closest_index] < 0.5) {
#     state_provences[i] <- cities_list$stateProvince[closest_index]
#   } else {
#     state_provences[i] <- NA 
#   }
# }
```
  Taxonomy Labels
```{r}

##### Renaming verbatim scientific name levels #####
lvl_verbatim = levels(data$verbatimScientificName)
verbatim_sp <- lapply(lvl_verbatim, function(x) word(x, 1, 2))
levels(data$verbatimScientificName) = unlist(verbatim_sp)

# add species information from verbatimScientificName 
data$species <- as.character(data$species)
data[which(data$species==""),"species"] <-   as.vector(data[(data$species==""),"verbatimScientificName"])
data$species <- as.factor(data$species)

# add genus information from species
gen <- lapply(data[data$genus=="" & !is.na(data$species),"species"], function(x) word(x, 1))

data$genus <- as.character(data$genus)
data[which(data$genus=="" & !is.na(data$species)),"genus"] <- as.vector(gen)
data[data$genus=="","genus"] <- NA
data$genus <- as.factor(data$genus)

# sanity checks
print(which(data$species=="")) # there shouldn't be any more empty species
print(which(data$genus=="")) # there shouldn't be any more empty genus

data = data %>% select(-c("verbatimScientificName"))
```
  Dates
```{r}
print("before")
print(sum(is.na(data$eventDate)))


data$eventDate <- replace(data$eventDate, is.na(data$eventDate), data$dateIdentified)

# remaining NAs
print("after")
print(sum(is.na(data$eventDate)))

# data = data %>% select(-c("dateIdentified"))
```

# Export Processed Data
```{r}
write.csv(data, "data_filled_gaps.csv", row.names=FALSE)
```
